# ğŸ§  GenAI App: LangChain + Ollama (LLama2) + FAISS

This project is a lightweight and functional GenAI app that performs retrieval-augmented generation (RAG) using LangChain. It loads data from a webpage, chunks it, embeds using `Ollama` (LLama2), stores the embeddings in FAISS, and answers questions contextually using a prompt-based LLM pipeline.

---

## ğŸš€ Features

- ğŸŒ Loads real-time web data with `WebBaseLoader`
- ğŸ”— LangChain-powered document chaining and prompting
- ğŸ§  LLama2 (via Ollama) for fast, local inference
- ğŸ§² FAISS vectorstore for semantic similarity search
- ğŸ—£ï¸ Retrieval-augmented QA system using a custom chat prompt

---


## ğŸ“Œ Tech Stack

- **LangChain** â€“ Chain documents & prompts
- **Ollama** â€“ Lightweight local LLM serving (LLama2)
- **FAISS** â€“ Vector search engine
- **OpenAI API (optional)** â€“ Fallback LLMs
- **Python** â€“ Orchestrating everything

---
